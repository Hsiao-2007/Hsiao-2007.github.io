{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hsiao-2007/Hsiao-2007.github.io/blob/main/MLB_Project_4_Main_FineTuning_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgOe3NtB-q2X"
      },
      "source": [
        "# MLB Project 4 Main - Fine-Tuning a Language Model\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "Welcome to the Fine-Tuning Project! In this project, you'll learn how to **fine-tune a pre-trained language model** for sentiment analysis on product reviews.\n",
        "\n",
        "### What is Fine-Tuning?\n",
        "Fine-tuning is the process of taking a pre-trained model and adapting it to a specific task by training it on task-specific data. This allows you to leverage the general language understanding the model has already learned while specializing it for your particular use case.\n",
        "\n",
        "### What You'll Learn\n",
        "- How to load and preprocess a dataset for fine-tuning\n",
        "- How to prepare a pre-trained model for a classification task\n",
        "- How to set up training arguments and optimize hyperparameters\n",
        "- How to train and evaluate a fine-tuned model\n",
        "- How to make predictions with your trained model\n",
        "\n",
        "### Project Structure\n",
        "1. Setup and imports\n",
        "2. Data loading and exploration\n",
        "3. Data preprocessing and tokenization\n",
        "4. Model preparation\n",
        "5. Training configuration\n",
        "6. Model training\n",
        "7. Evaluation and inference\n",
        "\n",
        "### Dataset: Amazon Product Reviews\n",
        "We'll use a subset of Amazon product reviews with ratings from 1-5 stars. Your task is to predict the sentiment (positive/negative/neutral) based on the review text.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkvDxZKt-q2Y"
      },
      "source": [
        "## Step 1: Setup and Imports\n",
        "\n",
        "First, let's install the required libraries and import them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "N_raVXQ2-q2Z"
      },
      "outputs": [],
      "source": [
        "# Install required packages (run this cell first!)\n",
        "!pip install transformers datasets torch scikit-learn accelerate evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "w9Sk_S1i-q2Z",
        "outputId": "a3f49127-3fdd-4fcd-e99c-a79d43eb3b7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import evaluate\n",
        "\n",
        "# Suppress unnecessary warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGKilY6Y-q2Z"
      },
      "source": [
        "## Step 2: Configuration\n",
        "\n",
        "Let's set up our model names and training configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "FaYaJfws-q2Z",
        "outputId": "e8de0438-06ec-4548-c9d3-0a9bc73b9852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: distilbert-base-uncased\n",
            "Max Length: 128\n",
            "Batch Size: 16\n",
            "Epochs: 3\n",
            "Learning Rate: 2e-05\n"
          ]
        }
      ],
      "source": [
        "# Configuration settings\n",
        "MODEL_NAME = \"distilbert-base-uncased\"  # A lighter, faster version of BERT\n",
        "MAX_LENGTH = 128  # Maximum sequence length for tokenization\n",
        "BATCH_SIZE = 16  # Number of samples per training batch\n",
        "NUM_EPOCHS = 3  # Number of training epochs\n",
        "LEARNING_RATE = 2e-5  # Learning rate for the optimizer\n",
        "\n",
        "# Label mapping\n",
        "LABEL_MAPPING = {\n",
        "    0: \"negative\",  # 1-2 star reviews\n",
        "    1: \"neutral\",   # 3 star reviews\n",
        "    2: \"positive\"   # 4-5 star reviews\n",
        "}\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Max Length: {MAX_LENGTH}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning Rate: {LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqaXXQ84-q2a"
      },
      "source": [
        "## Step 3: Load and Explore the Dataset\n",
        "\n",
        "We'll use the Amazon Polarity dataset which contains product reviews. We'll convert the ratings into sentiment labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYncA5dj-q2a"
      },
      "source": [
        "### 3.1: Load the Dataset\n",
        "\n",
        "**TODO**: Load the dataset and take a smaller subset for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "XGjPWOwp-q2a",
        "outputId": "98bceb3d-b731-400f-ced1-5dccfac27dd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Dataset loaded with 5000 samples\n",
            "\n",
            "Dataset structure: Dataset({\n",
            "    features: ['label', 'title', 'content'],\n",
            "    num_rows: 5000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading dataset...\")\n",
        "\n",
        "# TODO: Load the amazon_polarity dataset\n",
        "dataset = load_dataset('fancyzhx/amazon_polarity', split=\"train\")  # Replace None with your code\n",
        "\n",
        "# We'll take a smaller subset for faster training (5000 samples)\n",
        "# TODO: Select the first 5000 samples from the dataset\n",
        "dataset = dataset.select(range(5000))  # Replace None with your code\n",
        "\n",
        "print(f\"Dataset loaded with {len(dataset)} samples\")\n",
        "print(f\"\\nDataset structure: {dataset}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmjkXXTS-q2a"
      },
      "source": [
        "### 3.2: Explore the Data\n",
        "\n",
        "Let's examine some sample reviews to understand the data better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "vTolhy7L-q2a",
        "outputId": "ebe7b76a-b321-4278-cfe7-226cdd4eb396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample reviews:\n",
            "\n",
            "Example 1:\n",
            "Title: Stuning even for the non-gamer\n",
            "Content: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I...\n",
            "Label: 1 (neutral)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 2:\n",
            "Title: The best soundtrack ever to anything.\n",
            "Content: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The ...\n",
            "Label: 1 (neutral)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 3:\n",
            "Title: Amazing!\n",
            "Content: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you've played the game) and the hope in \"A Distant Promise\" and \"G...\n",
            "Label: 1 (neutral)\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: Display the first 3 examples from the dataset\n",
        "print(\"Sample reviews:\\n\")\n",
        "for i in range(3):\n",
        "    sample = dataset[i]  # Replace None with code to get sample i\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"Title: {sample['title']}\")\n",
        "    print(f\"Content: {sample['content'][:200]}...\")  # Show first 200 chars\n",
        "    print(f\"Label: {sample['label']} ({LABEL_MAPPING[sample['label']]})\")\n",
        "    print(\"-\" * 80)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h2DF0Nc-q2a"
      },
      "source": [
        "### 3.3: Prepare Labels\n",
        "\n",
        "The amazon_polarity dataset has binary labels (0=negative, 1=positive). For this project, we'll work with these as-is, but you could extend this to include neutral sentiment.\n",
        "\n",
        "**TODO**: Create a function to combine title and content into a single text field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "DUJIbgLu-q2a",
        "outputId": "835d8400-4a9d-4338-bfce-2ea4a3ede817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text preparation complete!\n",
            "\n",
            "Updated dataset structure: Dataset({\n",
            "    features: ['label', 'text'],\n",
            "    num_rows: 5000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def prepare_text(example):\n",
        "    \"\"\"\n",
        "    Combine title and content into a single text field.\n",
        "\n",
        "    Args:\n",
        "        example: A single example from the dataset\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with 'text' and 'label' fields\n",
        "    \"\"\"\n",
        "    # TODO: Combine title and content with a separator\n",
        "    text = example['title'] + '|' + example['content']  # Replace None with your code\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'label': example['label']\n",
        "    }\n",
        "\n",
        "# TODO: Apply the prepare_text function to the entire dataset\n",
        "dataset = dataset.map(prepare_text) # Replace None with your code\n",
        "dataset = dataset.remove_columns(['title','content'])\n",
        "\n",
        "print(\"Text preparation complete!\")\n",
        "print(f\"\\nUpdated dataset structure: {dataset}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhj0MEvD-q2a"
      },
      "source": [
        "### 3.4: Split the Dataset\n",
        "\n",
        "**TODO**: Split the dataset into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2fOaWhDe-q2a",
        "outputId": "f7dd2c0b-5781-41cc-907d-086310896dd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split complete!\n",
            "\n",
            "Dataset splits:\n",
            "   Training: 3500 samples\n",
            "   Validation: 750 samples\n",
            "   Test: 750 samples\n"
          ]
        }
      ],
      "source": [
        "# TODO: Split the dataset into train (70%), validation (15%), and test (15%)\n",
        "\n",
        "# First split: 70% train, 30% temp\n",
        "train_test =  dataset.train_test_split(train_size=0.7) # Replace None with your code\n",
        "\n",
        "# Second split: Split the temp set into 50% validation, 50% test\n",
        "val_test = train_test['test'].train_test_split(train_size=0.5)  # Replace None with your code\n",
        "\n",
        "# Create the final dataset dictionary\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_test['train'],\n",
        "    'validation': val_test['train'],\n",
        "    'test': val_test['test']\n",
        "})\n",
        "\n",
        "print(\"Dataset split complete!\")\n",
        "print(f\"\\nDataset splits:\")\n",
        "print(f\"   Training: {len(dataset_dict['train'])} samples\")\n",
        "print(f\"   Validation: {len(dataset_dict['validation'])} samples\")\n",
        "print(f\"   Test: {len(dataset_dict['test'])} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP9iR8zy-q2b"
      },
      "source": [
        "## Step 4: Tokenization\n",
        "\n",
        "Tokenization converts text into numerical tokens that the model can understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNKZ6xrV-q2b"
      },
      "source": [
        "### 4.1: Load the Tokenizer\n",
        "\n",
        "**TODO**: Load the tokenizer for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "aNgYrC6o-q2b",
        "outputId": "953f36f4-238f-46e4-b74b-3cedd5bf3ca3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n",
            "Tokenizer loaded: DistilBertTokenizerFast\n",
            "Vocabulary size: 30522\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading tokenizer...\")\n",
        "\n",
        "# TODO: Load the tokenizer using AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)  # Replace None with your code\n",
        "\n",
        "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTyv3OzC-q2b"
      },
      "source": [
        "### 4.2: Create Tokenization Function\n",
        "\n",
        "**TODO**: Create a function to tokenize the text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "s0D4WRVF-q2b",
        "outputId": "14c6f20b-6fb8-471d-c486-1b1c628250ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408,
          "referenced_widgets": [
            "ad651f67ac824e75bce3d05a89657e44",
            "9701efc904f749ad99860719b57ac7d1",
            "a1d99e3da1074c19bb11834b1768cd4c",
            "6b0fb5f625cc40d095df63d7721a0bad",
            "3c310e97d60447829472c18654060744",
            "cd5ebc97a5434597b456553b3fe6d498",
            "8cc92ed0f2ca40508bc122fd2d9f2358",
            "0e32c8f7cd754ad681947fbaf145f0eb",
            "2fa6e0e0a03a4bc4bda1f80101ee45d0",
            "6a031786c0e14bd2a149a9fbf7780ebc",
            "616448c49b3649d196e3661bd3ca191f",
            "60cbd65651cd4e35bd5a92ae685c7ead",
            "449736fe0ca848a9a19b3ec6f6092591",
            "b81a6c1d7c8348d2943b3c6fef6c04b9",
            "ad66b12947374c21a03169b60e0ac7ce",
            "42815554bf2b43e2b52ba130df35c5d5",
            "4f53b99115ba4d9a8fa63592a4e88cfb",
            "77d13371d9fc41d1a405697b898df6da",
            "761db0cf3cb5489183d04440a9ae6294",
            "5abd2b69371e4e8a9d1a9d0c947fb05a",
            "cfa02eb6b99e4d92a535ab6d669f8e59",
            "8af1e7789c4e4b3cb2ba9d16a984e0ac",
            "7cf3e58af883422db753f50759877910",
            "4aa1ddcd317d4cdfad7350677e042f76",
            "57a361aa1c5b47cdb2192b234a87b9c8",
            "a2921093720e498aa340bf9b995b1667",
            "b0c3eccc39504b44a7273adf05394c4a",
            "0ca93c5ed5c4488b8109fbe2e96c51e7",
            "40405703ac4040fa8807f94318ef8f23",
            "d883d3df4fed4b29a00c722877363231",
            "65865e4cb3da4f839565b155b3085199",
            "d24009bb586c4a5eb617e1530aa1700d",
            "74543624b3c3471aa7f83dabacc243d4"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing datasets...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad651f67ac824e75bce3d05a89657e44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/750 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60cbd65651cd4e35bd5a92ae685c7ead"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/750 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cf3e58af883422db753f50759877910"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization complete!\n",
            "\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 3500\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 750\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 750\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenize the text in the examples.\n",
        "\n",
        "    Args:\n",
        "        examples: Batch of examples from the dataset\n",
        "\n",
        "    Returns:\n",
        "        Tokenized examples\n",
        "    \"\"\"\n",
        "    # TODO: Tokenize the text with padding and truncation\n",
        "    return tokenizer(examples['text'], max_length=MAX_LENGTH, truncation=True, padding=True) # Replace None with your code\n",
        "\n",
        "# TODO: Apply tokenization to all splits\n",
        "print(\"Tokenizing datasets...\")\n",
        "tokenized_datasets = dataset_dict.map(tokenize_function) # Replace None with your code\n",
        "\n",
        "print(\"Tokenization complete!\")\n",
        "print(f\"\\nTokenized dataset structure: {tokenized_datasets}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD1tWlcV-q2b"
      },
      "source": [
        "### 4.3: Verify Tokenization\n",
        "\n",
        "Let's check that tokenization worked correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "k-bEKqQo-q2b",
        "outputId": "a3159a90-1b59-4db3-f33f-da068d6faee2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample tokenized example:\n",
            "\n",
            "Input IDs shape: 118\n",
            "Input IDs (first 20): [101, 6297, 8428, 4063, 2003, 6297, 8428, 4667, 999, 1064, 4406, 1996, 2034, 2048, 2808, 1999, 1996, 2186, 6297, 8428]\n",
            "\n",
            "Decoded text: [CLS] spellbinder is spellbinding! | unlike the first two books in the series spellbinder switches it ' s romantic connections to witches insted of vampires. blaise uses human men as toys till they go crazy, and when she sets...\n",
            "Label: 1\n"
          ]
        }
      ],
      "source": [
        "# Examine a tokenized example\n",
        "print(\"Sample tokenized example:\\n\")\n",
        "sample = tokenized_datasets['train'][0]\n",
        "print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
        "print(f\"Input IDs (first 20): {sample['input_ids'][:20]}\")\n",
        "print(f\"\\nDecoded text: {tokenizer.decode(sample['input_ids'][:50])}...\")\n",
        "print(f\"Label: {sample['label']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xAzN3gD-q2b"
      },
      "source": [
        "## Step 5: Prepare the Model\n",
        "\n",
        "Now we'll load the pre-trained model and prepare it for our classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLJygPdj-q2b"
      },
      "source": [
        "### 5.1: Load the Model\n",
        "\n",
        "**TODO**: Load the pre-trained model for sequence classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "GGpJy7aq-q2b",
        "outputId": "282742fa-5244-4048-aebd-e52ec2ebae81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-trained model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded: DistilBertForSequenceClassification\n",
            "Total parameters: 66,955,010\n",
            "Trainable parameters: 66,955,010\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading pre-trained model...\")\n",
        "\n",
        "# TODO: Load the model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)  # Replace None with your code\n",
        "\n",
        "print(f\"Model loaded: {model.__class__.__name__}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT8q7JQx-q2b"
      },
      "source": [
        "### 5.2: Create Data Collator\n",
        "\n",
        "A data collator handles batching and padding during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "abrFH5Kh-q2b",
        "outputId": "8491f67a-df43-4f8a-8f11-42c05b19b087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collator created!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a data collator with padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer)  # Replace None with your code\n",
        "\n",
        "print(\"Data collator created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqU8gNTG-q2c"
      },
      "source": [
        "## Step 6: Define Evaluation Metrics\n",
        "\n",
        "We need to define how to evaluate our model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "W9K7y7z1-q2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae4efec-b7c8-4e23-9e78-3f203e91ef30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics function defined!\n"
          ]
        }
      ],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics for the model.\n",
        "\n",
        "    Args:\n",
        "        eval_pred: Tuple of (predictions, labels)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of metric values\n",
        "    \"\"\"\n",
        "    # TODO: Extract predictions and labels\n",
        "    predictions, labels = eval_pred[0], eval_pred[1]  # Replace None with your code\n",
        "\n",
        "    # TODO: Get predicted class by taking argmax\n",
        "    print(type(predictions[0][0]))\n",
        "    predictions = [torch.argmax(i) for i in predictions] # Replace None with your code\n",
        "\n",
        "    # TODO: Calculate accuracy\n",
        "    accuracy = accuracy_score(predictions, labels) # Replace None with your code\n",
        "\n",
        "    # TODO: Calculate precision, recall, and F1\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(predictions, labels)  # Replace with your code\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "print(\"Metrics function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqrEFfF_-q2c"
      },
      "source": [
        "## Step 7: Set Up Training Arguments\n",
        "\n",
        "Training arguments control how the model is trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "KQ8jbSBL-q2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a9a9085-8ba1-44ef-b18a-e6b5996e0c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments configured!\n",
            "\n",
            "Training configuration:\n",
            "Epochs: 3\n",
            "Batch size: 16\n",
            "Learning rate: 2e-05\n"
          ]
        }
      ],
      "source": [
        "# TODO: Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",              # Output directory for checkpoints\n",
        "    num_train_epochs=NUM_EPOCHS,               # TODO\n",
        "    per_device_train_batch_size=BATCH_SIZE,    # TODO\n",
        "    per_device_eval_batch_size=BATCH_SIZE,     # TODO\n",
        "    learning_rate=LEARNING_RATE,                  # TODO\n",
        "    weight_decay=0.01,                   # Regularization\n",
        "    eval_strategy=\"epoch\",               # Evaluate after each epoch\n",
        "    save_strategy=\"epoch\",               # Save checkpoint after each epoch\n",
        "    load_best_model_at_end=True,         # Load best model at the end\n",
        "    metric_for_best_model=\"accuracy\",    # Use accuracy to determine best model\n",
        "    logging_dir=\"./logs\",                # TensorBoard logs\n",
        "    logging_steps=50,                    # Log every 50 steps\n",
        "    warmup_steps=100,                    # Warmup steps for learning rate\n",
        "    seed=42,                             # Random seed\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured!\")\n",
        "print(f\"\\nTraining configuration:\")\n",
        "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Learning rate: {training_args.learning_rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmCA28RY-q2c"
      },
      "source": [
        "## Step 8: Create the Trainer\n",
        "\n",
        "The Trainer handles the training loop, evaluation, and logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "LDljAFhM-q2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99360433-efc3-41d1-fcad-17c95ccb0691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer created!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a Trainer instance\n",
        "trainer = Trainer(model,\n",
        "                  data_collator=data_collator,\n",
        "                  train_dataset=tokenized_datasets['train'],\n",
        "                  compute_metrics=compute_metrics,\n",
        "                  args=training_args,\n",
        "                  eval_dataset=tokenized_datasets['test']\n",
        ")  # Replace None with your code\n",
        "\n",
        "print(\"Trainer created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xejdr6UO-q2c"
      },
      "source": [
        "## Step 9: Train the Model\n",
        "\n",
        "Now we're ready to train! This will take several minutes.\n",
        "\n",
        "**TODO**: Start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "3QFqibL8-q2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "b0612a3d-0e7b-48c8-aba6-f71d8d97eafe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='220' max='657' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [220/657 00:37 < 01:15, 5.77 it/s, Epoch 1/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [47/47 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.float32'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1300809174.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# TODO: Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace None with your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2790\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2791\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3219\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3221\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3222\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3170\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4488\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4489\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4490\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4491\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4778\u001b[0m             \u001b[0meval_set_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"losses\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_losses\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"loss\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4779\u001b[0m             \u001b[0meval_set_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_inputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"inputs\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4780\u001b[0;31m             metrics = self.compute_metrics(\n\u001b[0m\u001b[1;32m   4781\u001b[0m                 \u001b[0mEvalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_set_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4782\u001b[0m             )\n",
            "\u001b[0;32m/tmp/ipython-input-1745731397.py\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# TODO: Get predicted class by taking argmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Replace None with your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# TODO: Calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
          ]
        }
      ],
      "source": [
        "print(\"Starting training...\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# TODO: Train the model\n",
        "train_result = trainer.train()  # Replace None with your code\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Training complete!\")\n",
        "print(f\"\\nTraining metrics:\")\n",
        "print(f\"Final loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31W4oVsh-q2c"
      },
      "source": [
        "## Step 10: Evaluate the Model\n",
        "\n",
        "Let's evaluate our fine-tuned model on the validation and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btIkxQPv-q2c"
      },
      "source": [
        "### 10.1: Validation Set Evaluation\n",
        "\n",
        "**TODO**: Evaluate on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UetrdVFd-q2c"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluating on validation set...\\n\")\n",
        "\n",
        "# TODO: Evaluate the model on validation set\n",
        "val_results = evaluate()  # Replace None with your code\n",
        "\n",
        "print(\"Validation results:\")\n",
        "for metric, value in val_results.items():\n",
        "    if metric.startswith('eval_'):\n",
        "        print(f\"   {metric[5:]}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7vhgGcW-q2c"
      },
      "source": [
        "### 10.2: Test Set Evaluation\n",
        "\n",
        "**TODO**: Evaluate on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7MDFP0I-q2c"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluating on test set...\\n\")\n",
        "\n",
        "# TODO: Evaluate the model on test set\n",
        "test_results = None  # Replace None with your code\n",
        "\n",
        "print(\"Test results:\")\n",
        "for metric, value in test_results.items():\n",
        "    if metric.startswith('eval_'):\n",
        "        print(f\"   {metric[5:]}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI5cTcfL-q2h"
      },
      "source": [
        "## Step 11: Make Predictions\n",
        "\n",
        "Now let's use our fine-tuned model to make predictions on new text!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OllyMZEt-q2h"
      },
      "source": [
        "### 11.1: Create Prediction Function\n",
        "\n",
        "**TODO**: Create a function to predict sentiment for new reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuqPS3QT-q2h"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text: str, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Predict the sentiment of a text review.\n",
        "\n",
        "    Args:\n",
        "        text: Review text to classify\n",
        "        model: Fine-tuned model\n",
        "        tokenizer: Tokenizer\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (predicted_label, confidence_score)\n",
        "    \"\"\"\n",
        "    # TODO: Tokenize the input text\n",
        "    inputs = tokenizer.tokenize(text)  # Replace None with your code\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # TODO: Get model predictions (no gradient calculation needed)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.predict(inputs)  # Replace None with your code\n",
        "\n",
        "    # TODO: Get the predicted class\n",
        "    predicted_class = None  # Replace None with your code\n",
        "\n",
        "    # TODO: Calculate confidence score using softmax\n",
        "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "    confidence = None  # Replace None with your code\n",
        "\n",
        "    # Map to label name\n",
        "    label_name = LABEL_MAPPING[predicted_class.item()]\n",
        "\n",
        "    return label_name, confidence.item()\n",
        "\n",
        "print(\"Prediction function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPUjrDrQ-q2h"
      },
      "source": [
        "### 11.2: Test with Sample Reviews\n",
        "\n",
        "Let's test our model with some example reviews!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQKhMZ3U-q2h"
      },
      "outputs": [],
      "source": [
        "# Sample reviews to test\n",
        "test_reviews = [\n",
        "    \"This product is amazing! Best purchase I've ever made. Highly recommend!\",\n",
        "    \"Terrible quality. Broke after one day. Complete waste of money.\",\n",
        "    \"This is absolutely the worst product I have ever purchased. Save your money!\",\n",
        "    \"Great value for the price. Works exactly as described.\",\n",
        "    \"Not bad, but not great either. It's okay for the price.\"\n",
        "]\n",
        "\n",
        "print(\"Making predictions on sample reviews:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, review in enumerate(test_reviews, 1):\n",
        "    # TODO: Get prediction for the review\n",
        "    sentiment, confidence = predict_sentiment(review[i], model, tokenizer) # Replace with your code\n",
        "\n",
        "    print(f\"\\nReview {i}: {review}\")\n",
        "    print(f\"Predicted Sentiment: {sentiment.upper()}\")\n",
        "    print(f\"Confidence: {confidence:.2%}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3LKxPBo-q2h"
      },
      "source": [
        "### 11.3: Interactive Prediction\n",
        "\n",
        "Try it yourself! Enter your own reviews to classify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYfWnqZt-q2h"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Interactive Sentiment Classifier\")\n",
        "print(\"Enter product reviews to classify their sentiment.\")\n",
        "print(\"Type 'exit' or 'quit' to stop.\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    review = input(\"\\n Enter a review: \")\n",
        "\n",
        "    # Check if user wants to exit\n",
        "    if review.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"\\n Thank you for using the sentiment classifier!\")\n",
        "        break\n",
        "\n",
        "    # Skip empty input\n",
        "    if not review.strip():\n",
        "        continue\n",
        "\n",
        "    # TODO: Get prediction\n",
        "    sentiment, confidence = predict_sentiment(review, model, tokenizer)  # Replace with your code\n",
        "\n",
        "    print(f\"\\n Prediction: {sentiment.upper()}\")\n",
        "    print(f\" Confidence: {confidence:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQxVYhkF-q2i"
      },
      "source": [
        "##  Congratulations!\n",
        "\n",
        "You've successfully fine-tuned a language model for sentiment analysis! Here's what you accomplished:\n",
        "\n",
        "1.  Loaded and preprocessed the Amazon product reviews dataset\n",
        "2.  Tokenized text data for model input\n",
        "3.  Configured and loaded a pre-trained DistilBERT model\n",
        "4.  Set up training arguments and evaluation metrics\n",
        "5.  Fine-tuned the model on sentiment classification\n",
        "6.  Evaluated model performance on validation and test sets\n",
        "7.  Created a prediction function for new reviews\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Want to improve your model? Try:\n",
        "- Training for more epochs\n",
        "- Experimenting with different learning rates\n",
        "- Using a larger model (e.g., BERT-base instead of DistilBERT)\n",
        "- Adding more training data\n",
        "- Implementing data augmentation techniques\n",
        "- Trying different optimizers or schedulers\n",
        "- Fine-tuning on a different task (e.g., multi-class classification)\n",
        "- Adding attention visualization to understand model decisions\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
        "- [Fine-tuning Guide](https://huggingface.co/docs/transformers/training)\n",
        "- [DistilBERT Paper](https://arxiv.org/abs/1910.01108)\n",
        "- [Transfer Learning in NLP](https://ruder.io/transfer-learning/)\n",
        "\n",
        "### Challenge Tasks\n",
        "\n",
        "1. **Hyperparameter Tuning**: Experiment with different batch sizes, learning rates, and epochs to improve performance\n",
        "2. **Model Comparison**: Try fine-tuning different models (BERT, RoBERTa, ALBERT) and compare results\n",
        "3. **Error Analysis**: Analyze misclassified examples to understand model limitations\n",
        "4. **Deployment**: Create a simple web app using Gradio or Streamlit to deploy your model\n",
        "\n",
        "Great work! "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad651f67ac824e75bce3d05a89657e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9701efc904f749ad99860719b57ac7d1",
              "IPY_MODEL_a1d99e3da1074c19bb11834b1768cd4c",
              "IPY_MODEL_6b0fb5f625cc40d095df63d7721a0bad"
            ],
            "layout": "IPY_MODEL_3c310e97d60447829472c18654060744"
          }
        },
        "9701efc904f749ad99860719b57ac7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd5ebc97a5434597b456553b3fe6d498",
            "placeholder": "",
            "style": "IPY_MODEL_8cc92ed0f2ca40508bc122fd2d9f2358",
            "value": "Map:100%"
          }
        },
        "a1d99e3da1074c19bb11834b1768cd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e32c8f7cd754ad681947fbaf145f0eb",
            "max": 3500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fa6e0e0a03a4bc4bda1f80101ee45d0",
            "value": 3500
          }
        },
        "6b0fb5f625cc40d095df63d7721a0bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a031786c0e14bd2a149a9fbf7780ebc",
            "placeholder": "",
            "style": "IPY_MODEL_616448c49b3649d196e3661bd3ca191f",
            "value": "3500/3500[00:12&lt;00:00,230.23examples/s]"
          }
        },
        "3c310e97d60447829472c18654060744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd5ebc97a5434597b456553b3fe6d498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cc92ed0f2ca40508bc122fd2d9f2358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e32c8f7cd754ad681947fbaf145f0eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fa6e0e0a03a4bc4bda1f80101ee45d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a031786c0e14bd2a149a9fbf7780ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "616448c49b3649d196e3661bd3ca191f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60cbd65651cd4e35bd5a92ae685c7ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_449736fe0ca848a9a19b3ec6f6092591",
              "IPY_MODEL_b81a6c1d7c8348d2943b3c6fef6c04b9",
              "IPY_MODEL_ad66b12947374c21a03169b60e0ac7ce"
            ],
            "layout": "IPY_MODEL_42815554bf2b43e2b52ba130df35c5d5"
          }
        },
        "449736fe0ca848a9a19b3ec6f6092591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f53b99115ba4d9a8fa63592a4e88cfb",
            "placeholder": "",
            "style": "IPY_MODEL_77d13371d9fc41d1a405697b898df6da",
            "value": "Map:100%"
          }
        },
        "b81a6c1d7c8348d2943b3c6fef6c04b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_761db0cf3cb5489183d04440a9ae6294",
            "max": 750,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5abd2b69371e4e8a9d1a9d0c947fb05a",
            "value": 750
          }
        },
        "ad66b12947374c21a03169b60e0ac7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa02eb6b99e4d92a535ab6d669f8e59",
            "placeholder": "",
            "style": "IPY_MODEL_8af1e7789c4e4b3cb2ba9d16a984e0ac",
            "value": "750/750[00:02&lt;00:00,387.55examples/s]"
          }
        },
        "42815554bf2b43e2b52ba130df35c5d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f53b99115ba4d9a8fa63592a4e88cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d13371d9fc41d1a405697b898df6da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "761db0cf3cb5489183d04440a9ae6294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5abd2b69371e4e8a9d1a9d0c947fb05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfa02eb6b99e4d92a535ab6d669f8e59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8af1e7789c4e4b3cb2ba9d16a984e0ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cf3e58af883422db753f50759877910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4aa1ddcd317d4cdfad7350677e042f76",
              "IPY_MODEL_57a361aa1c5b47cdb2192b234a87b9c8",
              "IPY_MODEL_a2921093720e498aa340bf9b995b1667"
            ],
            "layout": "IPY_MODEL_b0c3eccc39504b44a7273adf05394c4a"
          }
        },
        "4aa1ddcd317d4cdfad7350677e042f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca93c5ed5c4488b8109fbe2e96c51e7",
            "placeholder": "",
            "style": "IPY_MODEL_40405703ac4040fa8807f94318ef8f23",
            "value": "Map:100%"
          }
        },
        "57a361aa1c5b47cdb2192b234a87b9c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d883d3df4fed4b29a00c722877363231",
            "max": 750,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65865e4cb3da4f839565b155b3085199",
            "value": 750
          }
        },
        "a2921093720e498aa340bf9b995b1667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d24009bb586c4a5eb617e1530aa1700d",
            "placeholder": "",
            "style": "IPY_MODEL_74543624b3c3471aa7f83dabacc243d4",
            "value": "750/750[00:02&lt;00:00,413.33examples/s]"
          }
        },
        "b0c3eccc39504b44a7273adf05394c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca93c5ed5c4488b8109fbe2e96c51e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40405703ac4040fa8807f94318ef8f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d883d3df4fed4b29a00c722877363231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65865e4cb3da4f839565b155b3085199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d24009bb586c4a5eb617e1530aa1700d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74543624b3c3471aa7f83dabacc243d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}